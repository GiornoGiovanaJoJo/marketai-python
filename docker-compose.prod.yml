# Production Docker Compose Configuration
# Использование: docker-compose -f docker-compose.prod.yml up -d

services:
  postgres:
    image: postgres:16-alpine
    container_name: marketai_postgres_prod
    environment:
      POSTGRES_DB: ${DB_NAME:-marketai}
      POSTGRES_USER: ${DB_USER:-marketai}
      POSTGRES_PASSWORD: ${DB_PASSWORD:?Database password required}
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --lc-collate=C --lc-ctype=C"
    volumes:
      - postgres_data_prod:/var/lib/postgresql/data
    networks:
      - marketai_network_prod
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-marketai}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  redis:
    image: redis:7-alpine
    container_name: marketai_redis_prod
    command: >
      redis-server
      --appendonly yes
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --requirepass ${REDIS_PASSWORD:?Redis password required}
    volumes:
      - redis_data_prod:/data
    networks:
      - marketai_network_prod
    healthcheck:
      test: ["CMD", "redis-cli", "--no-auth-warning", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: production
    container_name: marketai_backend_prod
    command: >
      sh -c "python manage.py migrate --noinput &&
             python manage.py collectstatic --noinput &&
             gunicorn core.wsgi:application
             --bind 0.0.0.0:8000
             --workers ${GUNICORN_WORKERS:-4}
             --threads ${GUNICORN_THREADS:-2}
             --timeout 120
             --max-requests 1000
             --max-requests-jitter 100
             --access-logfile /app/logs/gunicorn-access.log
             --error-logfile /app/logs/gunicorn-error.log
             --log-level info
             --worker-class gthread
             --worker-tmp-dir /dev/shm"
    volumes:
      - static_volume_prod:/app/staticfiles
      - media_volume_prod:/app/media
      - duckdb_data_prod:/app/data
      - logs_volume_prod:/app/logs
    expose:
      - "8000"
    env_file:
      - .env.production
    environment:
      - PYTHONUNBUFFERED=1
      - DJANGO_SETTINGS_MODULE=core.settings
      - DJANGO_PRODUCTION=1
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - marketai_network_prod
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
      replicas: 2
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  celery_worker:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: production
    container_name: marketai_celery_worker_prod
    command: >
      celery -A core worker
      --loglevel=info
      --concurrency=${CELERY_WORKERS:-4}
      --max-tasks-per-child=1000
      --time-limit=300
      --soft-time-limit=240
    volumes:
      - duckdb_data_prod:/app/data
      - logs_volume_prod:/app/logs
    env_file:
      - .env.production
    environment:
      - PYTHONUNBUFFERED=1
      - C_FORCE_ROOT=true
      - DJANGO_PRODUCTION=1
    depends_on:
      - backend
      - redis
      - postgres
    networks:
      - marketai_network_prod
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
      replicas: 2
    healthcheck:
      test: ["CMD-SHELL", "celery -A core inspect ping"]
      interval: 60s
      timeout: 20s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  celery_beat:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: production
    container_name: marketai_celery_beat_prod
    command: >
      celery -A core beat
      --loglevel=info
      --scheduler django_celery_beat.schedulers:DatabaseScheduler
      --pidfile=/tmp/celerybeat.pid
    volumes:
      - logs_volume_prod:/app/logs
    env_file:
      - .env.production
    environment:
      - PYTHONUNBUFFERED=1
      - DJANGO_PRODUCTION=1
    depends_on:
      - backend
      - redis
      - postgres
    networks:
      - marketai_network_prod
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      target: production
    container_name: marketai_frontend_prod
    expose:
      - "80"
    environment:
      - NODE_ENV=production
    depends_on:
      - backend
    networks:
      - marketai_network_prod
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  nginx:
    image: nginx:1.27-alpine
    container_name: marketai_nginx_prod
    ports:
      - "${NGINX_PORT:-80}:80"
      - "${NGINX_SSL_PORT:-443}:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - static_volume_prod:/static:ro
      - media_volume_prod:/media:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - logs_volume_prod:/var/log/nginx
    depends_on:
      - backend
      - frontend
    networks:
      - marketai_network_prod
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

volumes:
  postgres_data_prod:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/postgres
  redis_data_prod:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/redis
  static_volume_prod:
    driver: local
  media_volume_prod:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/media
  duckdb_data_prod:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/duckdb
  logs_volume_prod:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data}/logs

networks:
  marketai_network_prod:
    driver: bridge
    ipam:
      config:
        - subnet: 172.29.0.0/16
    driver_opts:
      com.docker.network.bridge.name: marketai_prod